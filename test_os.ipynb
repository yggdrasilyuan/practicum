{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ndjson\n",
    "import fnmatch\n",
    "\n",
    "file_list = os.listdir('F:\\\\data_remain\\\\')\n",
    "# file_list\n",
    "\n",
    "# path=[]\n",
    "# for i in file_list:\n",
    "#     path.append('F:\\\\data\\\\'+os.path.join(i)+'\\\\en-US')\n",
    "# print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are RO country's data: \n",
      "In this country RO has 484 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 11\n",
      "origin_line_number: 204\n",
      "drop_duplicate_lines_number: 109\n",
      "\n",
      "\n",
      "\n",
      "These are RS country's data: \n",
      "In this country RS has 228 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 76\n",
      "drop_duplicate_lines_number: 26\n",
      "\n",
      "\n",
      "\n",
      "These are RU country's data: \n",
      "In this country RU has 1424 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 2\n",
      "origin_line_number: 603\n",
      "drop_duplicate_lines_number: 262\n",
      "\n",
      "\n",
      "\n",
      "These are RW country's data: \n",
      "In this country RW has 72 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 31\n",
      "drop_duplicate_lines_number: 14\n",
      "\n",
      "\n",
      "\n",
      "These are SA country's data: \n",
      "In this country SA has 652 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 4\n",
      "origin_line_number: 205\n",
      "drop_duplicate_lines_number: 58\n",
      "\n",
      "\n",
      "\n",
      "These are SC country's data: \n",
      "In this country SC has 24 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 12\n",
      "drop_duplicate_lines_number: 6\n",
      "\n",
      "\n",
      "\n",
      "These are SD country's data: \n",
      "In this country SD has 64 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 22\n",
      "drop_duplicate_lines_number: 8\n",
      "\n",
      "\n",
      "\n",
      "These are SE country's data: \n",
      "In this country SE has 4884 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 1\n",
      "origin_line_number: 2605\n",
      "drop_duplicate_lines_number: 1359\n",
      "\n",
      "\n",
      "\n",
      "These are SG country's data: \n",
      "In this country SG has 2212 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 15\n",
      "origin_line_number: 1729\n",
      "drop_duplicate_lines_number: 837\n",
      "\n",
      "\n",
      "\n",
      "These are SI country's data: \n",
      "In this country SI has 340 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 2\n",
      "origin_line_number: 127\n",
      "drop_duplicate_lines_number: 55\n",
      "\n",
      "\n",
      "\n",
      "These are SK country's data: \n",
      "In this country SK has 244 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 2\n",
      "origin_line_number: 74\n",
      "drop_duplicate_lines_number: 21\n",
      "\n",
      "\n",
      "\n",
      "These are SL country's data: \n",
      "In this country SL has 32 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 9\n",
      "drop_duplicate_lines_number: 2\n",
      "\n",
      "\n",
      "\n",
      "These are SM country's data: \n",
      "In this country SM has 12 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 3\n",
      "drop_duplicate_lines_number: 0\n",
      "\n",
      "\n",
      "\n",
      "These are SN country's data: \n",
      "In this country SN has 28 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 7\n",
      "drop_duplicate_lines_number: 0\n",
      "\n",
      "\n",
      "\n",
      "These are SR country's data: \n",
      "In this country SR has 4 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 1\n",
      "drop_duplicate_lines_number: 0\n",
      "\n",
      "\n",
      "\n",
      "These are ST country's data: \n",
      "In this country ST has 4 files\n",
      "How many files has no RF: 1\n",
      "Json_error_number: 0\n",
      "origin_line_number: 0\n",
      "drop_duplicate_lines_number: 0\n",
      "\n",
      "\n",
      "\n",
      "These are SV country's data: \n",
      "In this country SV has 12 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 3\n",
      "drop_duplicate_lines_number: 0\n",
      "\n",
      "\n",
      "\n",
      "These are SY country's data: \n",
      "In this country SY has 64 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 17\n",
      "drop_duplicate_lines_number: 0\n",
      "\n",
      "\n",
      "\n",
      "These are SZ country's data: \n",
      "In this country SZ has 36 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 8\n",
      "drop_duplicate_lines_number: 0\n",
      "\n",
      "\n",
      "\n",
      "These are TC country's data: \n",
      "In this country TC has 16 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 4\n",
      "drop_duplicate_lines_number: 0\n",
      "\n",
      "\n",
      "\n",
      "These are TD country's data: \n",
      "In this country TD has 4 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 1\n",
      "drop_duplicate_lines_number: 0\n",
      "\n",
      "\n",
      "\n",
      "These are TG country's data: \n",
      "In this country TG has 84 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 36\n",
      "drop_duplicate_lines_number: 19\n",
      "\n",
      "\n",
      "\n",
      "These are TH country's data: \n",
      "In this country TH has 2828 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 19\n",
      "origin_line_number: 4546\n",
      "drop_duplicate_lines_number: 3439\n",
      "\n",
      "\n",
      "\n",
      "These are TJ country's data: \n",
      "In this country TJ has 16 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 2\n",
      "drop_duplicate_lines_number: 0\n",
      "\n",
      "\n",
      "\n",
      "These are TM country's data: \n",
      "In this country TM has 36 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 9\n",
      "drop_duplicate_lines_number: 0\n",
      "\n",
      "\n",
      "\n",
      "These are TN country's data: \n",
      "In this country TN has 100 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 1\n",
      "origin_line_number: 20\n",
      "drop_duplicate_lines_number: 0\n",
      "\n",
      "\n",
      "\n",
      "These are TR country's data: \n",
      "In this country TR has 796 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 230\n",
      "drop_duplicate_lines_number: 50\n",
      "\n",
      "\n",
      "\n",
      "These are TT country's data: \n",
      "In this country TT has 212 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 70\n",
      "drop_duplicate_lines_number: 26\n",
      "\n",
      "\n",
      "\n",
      "These are TW country's data: \n",
      "In this country TW has 1152 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 4\n",
      "origin_line_number: 542\n",
      "drop_duplicate_lines_number: 266\n",
      "\n",
      "\n",
      "\n",
      "These are TZ country's data: \n",
      "In this country TZ has 220 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 91\n",
      "drop_duplicate_lines_number: 45\n",
      "\n",
      "\n",
      "\n",
      "These are UA country's data: \n",
      "In this country UA has 292 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 103\n",
      "drop_duplicate_lines_number: 41\n",
      "\n",
      "\n",
      "\n",
      "These are UG country's data: \n",
      "In this country UG has 156 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 1\n",
      "origin_line_number: 49\n",
      "drop_duplicate_lines_number: 15\n",
      "\n",
      "\n",
      "\n",
      "These are US country's data: \n",
      "In this country US has 9780 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 19\n",
      "origin_line_number: 7764\n",
      "drop_duplicate_lines_number: 3983\n",
      "\n",
      "\n",
      "\n",
      "These are UY country's data: \n",
      "In this country UY has 36 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 15\n",
      "drop_duplicate_lines_number: 8\n",
      "\n",
      "\n",
      "\n",
      "These are UZ country's data: \n",
      "In this country UZ has 88 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 1\n",
      "origin_line_number: 20\n",
      "drop_duplicate_lines_number: 0\n",
      "\n",
      "\n",
      "\n",
      "These are VE country's data: \n",
      "In this country VE has 24 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 10\n",
      "drop_duplicate_lines_number: 5\n",
      "\n",
      "\n",
      "\n",
      "These are VG country's data: \n",
      "In this country VG has 372 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 208\n",
      "drop_duplicate_lines_number: 132\n",
      "\n",
      "\n",
      "\n",
      "These are VN country's data: \n",
      "In this country VN has 704 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 1\n",
      "origin_line_number: 221\n",
      "drop_duplicate_lines_number: 63\n",
      "\n",
      "\n",
      "\n",
      "These are VU country's data: \n",
      "In this country VU has 8 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 2\n",
      "drop_duplicate_lines_number: 0\n",
      "\n",
      "\n",
      "\n",
      "These are WS country's data: \n",
      "In this country WS has 8 files\n",
      "How many files has no RF: 1\n",
      "Json_error_number: 0\n",
      "origin_line_number: 0\n",
      "drop_duplicate_lines_number: 0\n",
      "\n",
      "\n",
      "\n",
      "These are YE country's data: \n",
      "In this country YE has 16 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 3\n",
      "drop_duplicate_lines_number: 0\n",
      "\n",
      "\n",
      "\n",
      "These are ZA country's data: \n",
      "In this country ZA has 4100 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 5\n",
      "origin_line_number: 2046\n",
      "drop_duplicate_lines_number: 916\n",
      "\n",
      "\n",
      "\n",
      "These are ZM country's data: \n",
      "In this country ZM has 96 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 25\n",
      "drop_duplicate_lines_number: 0\n",
      "\n",
      "\n",
      "\n",
      "These are ZW country's data: \n",
      "In this country ZW has 380 files\n",
      "How many files has no RF: 0\n",
      "Json_error_number: 0\n",
      "origin_line_number: 120\n",
      "drop_duplicate_lines_number: 37\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in file_list:\n",
    "    # folder with the Reference, Data, Word Count, and Sentiment zipped files\n",
    "    # os.chdir('C:\\\\vscode\\\\code\\\\practicum\\\\JSON_Data')\n",
    "    os.chdir('F:\\\\data_remain\\\\'+os.path.join(i)+'\\\\en-US')\n",
    "\n",
    "    flag=0\n",
    "    flag_empty=0\n",
    "    #### Filing Reference Package ####\n",
    "    # create an empty list, combined_ref, and loop through each file and combine to combined_ref\n",
    "    # combined ref will be a list of dictionaries\n",
    "    combined_ref = []\n",
    "    for file in os.listdir():\n",
    "        if(fnmatch.fnmatch(file, 'SP_FILING_REFERENCE*')):\n",
    "            with open(file,'r',encoding='utf_8') as ref:\n",
    "                ref = ndjson.load(ref)\n",
    "            combined_ref.extend(ref)    \n",
    "    \n",
    "    # normalize the reference package from its NDJSON form into a Data Frame - uses from pandas.io.json import json_normalize\n",
    "    ref_normalize = pd.json_normalize(combined_ref)\n",
    "\n",
    "    # change order of columns to reflect User Guide documentation\n",
    "    ref_normalize = ref_normalize[['ID', 'DOCUMENT_ID','DOCUMENT_TYPE','FILING_DATE','MODIFIED_AT','DETAIL_JSON.company_name', 'DETAIL_JSON.parsing_status']]\n",
    "\n",
    "    # convert the ID and document ID field to a string\n",
    "    ref_normalize['ID'] = ref_normalize['ID'].apply(str)\n",
    "    ref_normalize['DOCUMENT_ID'] = ref_normalize['DOCUMENT_ID'].apply(str)\n",
    "    # ref_normalize['DETAIL_JSON.ISIN_active'] = ref_normalize['DETAIL_JSON.ISIN_active'].apply(str)\n",
    "    # ref_normalize['DETAIL_JSON.SP_DocumentId'] = ref_normalize['DETAIL_JSON.SP_DocumentId'].apply(str)\n",
    "\n",
    "    #### Sentiment Score Package ####\n",
    "    # create an empty list, combined_sc, and loop through each file and combine to combined_sc\n",
    "    # combined_sc will be a list of dictionaries\n",
    "    combined_sc = []\n",
    "    for file in os.listdir():\n",
    "        if(fnmatch.fnmatch(file, 'SP_FILING_SENTIMENT*')):\n",
    "            try:\n",
    "                with open(file,'r',encoding='utf_8') as sc:# gzip 用来打开压缩文件中的数据\n",
    "                    sc = ndjson.load(sc)\n",
    "                combined_sc.extend(sc)  # 加入最后\n",
    "            except Exception as e:\n",
    "                pass\n",
    "                flag+=1\n",
    "            continue\n",
    "\n",
    "\n",
    "    ## Filing Level Function\n",
    "    # Function for calculating the sentiment figures at the main filing level - i.e., the aggregated sentiment figures for the actual\n",
    "    # filing (EXCLUDING exhibits)\n",
    "    #Parameter: filing_type - the filing type of the data in combined_sc (examples: '10-k', '10-q')\n",
    "    def filing_level_func(filing_type):\n",
    "    # column names of sentiment scores package based on User Guide documentation\n",
    "        column_names = ['ID','avg_sent','sum_sent','hit_count','positive_hits','negative_hits','section_count','word_count']\n",
    "    # create empty list called filin g_level_list\n",
    "        filing_level_list = [[]]\n",
    "    # loop through each element in combined_sc, check if the filing_type key exists in the 'SENTIMENT' key\n",
    "    # if it does append doc_level_list with ID and the sentiment scores\n",
    "    # if it does not, append doc_level_level with ID and NAs\n",
    "        for element in range(len(combined_sc)):\n",
    "            if combined_sc[element].get('SENTIMENT',{}).get(filing_type):\n",
    "                filing_level_list.append([str(combined_sc[element]['ID']), \n",
    "                            combined_sc[element]['SENTIMENT'][filing_type]['avg_sent'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type]['sum_sent'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type]['hit_count'], \n",
    "                            combined_sc[element]['SENTIMENT'][filing_type]['positive_hits'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type]['negative_hits'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type]['section_count'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type]['word_count']])\n",
    "            else:\n",
    "                filing_level_list.append([str(combined_sc[element]['ID']), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,np.nan])\n",
    "\n",
    "        # take contents of filing_level_list and put into a data frame called filing_level_df\n",
    "        filing_level_df = pd.DataFrame(filing_level_list[1:len(filing_level_list)], columns = column_names)    \n",
    "        \n",
    "        # convert the ID field to a string\n",
    "        filing_level_df['ID'] = filing_level_df['ID'].apply(str)\n",
    "        \n",
    "        # return the data frame\n",
    "        return filing_level_df\n",
    "\n",
    "\n",
    "    ## Section Level Function\n",
    "    # Function for calculating the sentiment figures at the section level\n",
    "    #Parameter: \n",
    "    # filing_type - the filing type of the data in combined_sc (examples: 'AR', 'QR', 'SR')\n",
    "    # section - the section of the filing_type of the data in combined_sc (examples: 'data', 'letter to shareholders', 'ceo report',etc)    \n",
    "    def section_level_func(filing_type, section):\n",
    "    # column names of sentiment scores package based on User Guide documentation\n",
    "        column_names = ['ID','avg_sent','sum_sent','hit_count','positive_hits','negative_hits','section_count','word_count']   \n",
    "    # create empty list called section_level_list\n",
    "        section_level_list = [[]] \n",
    "    # loop through each element in combined_sc, check if the section key exists in the 'SENTIMENT' key -> filing_type key\n",
    "    # if it does append section_level_list with ID and the sentiment scores\n",
    "    # if it does not, append section_level_level with ID and NAs\n",
    "        for element in range(len(combined_sc)):\n",
    "            if combined_sc[element].get('SENTIMENT',{}).get(filing_type,{}).get(section):\n",
    "                section_level_list.append([str(combined_sc[element]['ID']), \n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section]['avg_sent'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section]['sum_sent'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section]['hit_count'], \n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section]['positive_hits'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section]['negative_hits'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section]['section_count'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section]['word_count']])\n",
    "            else:\n",
    "                section_level_list.append([str(combined_sc[element]['ID']), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,np.nan])\n",
    "\n",
    "        # take contents of section_level_list and put into a data frame called section_level_df\n",
    "        section_level_df = pd.DataFrame(section_level_list[1:len(section_level_list)], columns = column_names)    \n",
    "        \n",
    "        # convert the ID field to a string\n",
    "        section_level_df['ID'] = section_level_df['ID'].apply(str)\n",
    "        \n",
    "        # return the data frame\n",
    "        return section_level_df\n",
    "\n",
    "\n",
    "    ## Sub-section Level Function  \n",
    "    #Parameter: \n",
    "    # filing_type - the filing type of the data in combined_sc (examples: '10-k', '10-q')\n",
    "    # section - the section of the filing_type of the data in combined_sc (examples: 'data', 'letter to shareholders', 'ceo report',etc)    \n",
    "    # sub-section - the sub-section of the section of the filing_type in combined_sc (examples: 'data','esg','risk', etc.)  \n",
    "    def sub_level_func(filing_type, section, sub):\n",
    "    # column names of sentiment scores package based on User Guide documentation\n",
    "        column_names = ['ID','avg_sent','sum_sent','hit_count','positive_hits','negative_hits','section_count','word_count']\n",
    "    # create empty list called sub_level_list\n",
    "        sub_level_list = [[]] \n",
    "    # loop through each element in combined_sc, check if the seub-section key exists in the 'SENTIMENT' key -> filing_type key -> section key\n",
    "    # if it does append sub_level_list with ID and the sentiment scores\n",
    "    # if it does not, append sub_level_list with ID and NAs\n",
    "        for element in range(len(combined_sc)):\n",
    "            if combined_sc[element].get('SENTIMENT',{}).get(filing_type,{}).get(section,{}).get(sub):\n",
    "                sub_level_list.append([str(combined_sc[element]['ID']), \n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section][sub]['avg_sent'], \n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section][sub]['sum_sent'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section][sub]['hit_count'], \n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section][sub]['positive_hits'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section][sub]['negative_hits'], \n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section][sub]['section_count'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section][sub]['word_count']])\n",
    "        \n",
    "            else: sub_level_list.append([str(combined_sc[element]['ID']), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,np.nan])\n",
    "            \n",
    "        # take contents of sub_level_list and put into a data frame called sub_level_df\n",
    "        sub_level_df = pd.DataFrame(sub_level_list[1:len(sub_level_list)], columns = column_names)    \n",
    "        \n",
    "        # convert the ID field to a string\n",
    "        sub_level_df['ID'] = sub_level_df['ID'].apply(str)\n",
    "        \n",
    "        # return the data frame\n",
    "        return sub_level_df\n",
    "\n",
    "    print(\"These are \"+i+\" country's data: \" )\n",
    "    ## Examples of calling functions\n",
    "    # get the AR sentiment data at the filing level\n",
    "    ars = filing_level_func('ar')\n",
    "    # rfs = section_level_func('ar','RF')\n",
    "    # rfs1 = sub_level_func('ar',section='data','data')\n",
    "    # if rfs.empty is True:\n",
    "    #     flag_empty+=1\n",
    "    # file_num= os.listdir()\n",
    "    # print(\"In this country \"+i+' has '+str(len(file_num))+\" files\")\n",
    "    print(\"How many files has no RF: \"+str(flag_empty))\n",
    "    # print(rfs)\n",
    "    # Join reference information with sentiment metrics\n",
    "    combined = pd.merge(left = ref_normalize, right = ars, on = 'ID')\n",
    "    # output= pd.merge(left = combined, right = rfs, on = 'ID')\n",
    "\n",
    "    print(\"Json_error_number: \"+str(flag))\n",
    "\n",
    "    # drop the duplicate \n",
    "    len_init=len(output)\n",
    "    print(\"origin_line_number: \"+str(len_init))\n",
    "    output.duplicated(['ID'])\n",
    "    output=output.drop_duplicates(['ID'])\n",
    "    len_out=len(output)\n",
    "    drop_len=len_init-len_out\n",
    "    print(\"drop_duplicate_lines_number: \"+str(drop_len))\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # print(output)\n",
    "    output.to_csv(\"F:\\\\outcome\\\\output_\"+i+\".csv\",sep=\",\",header=True)\n",
    "\n",
    "    # combined.groupy({})\n",
    "    # type(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir('F:\\\\data_remain\\\\')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b6a258fb8a3eb9173a5417bfda6a71d178e4756e23510b1388638bd90f2a1e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
