{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/T7 Touch//REFERENCE'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ndjson\n",
    "import os\n",
    "import gzip\n",
    "import fnmatch\n",
    "\n",
    "\n",
    "# folder with the Reference, Data, Word Count, and Sentiment zipped files\n",
    "# os.chdir('D:\\\\CA\\\\en-US')\n",
    "path=\"/Volumes/T7 Touch/\"\n",
    "reference_path=path+\"/REFERENCE\"\n",
    "sent_path=path+\"/SENTIMENT\"\n",
    "flag=0\n",
    "\n",
    "reference_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>DOCUMENT_TYPE</th>\n",
       "      <th>FILING_DATE</th>\n",
       "      <th>MODIFIED_AT</th>\n",
       "      <th>DETAIL_JSON.company_name</th>\n",
       "      <th>DETAIL_JSON.ISIN</th>\n",
       "      <th>DETAIL_JSON.ISIN_active</th>\n",
       "      <th>DETAIL_JSON.SP_DocumentId</th>\n",
       "      <th>DETAIL_JSON.parsing_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2533588</td>\n",
       "      <td>138980280.0</td>\n",
       "      <td>AR</td>\n",
       "      <td>2020-06-16</td>\n",
       "      <td>2021-05-16 09:34:00.000</td>\n",
       "      <td>Huisheng International Holdings Limited</td>\n",
       "      <td>KYG4643W1078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1028107306.0</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4036677</td>\n",
       "      <td>152637912.0</td>\n",
       "      <td>AR</td>\n",
       "      <td>2021-02-05</td>\n",
       "      <td>2021-05-16 09:34:00.000</td>\n",
       "      <td>Allied Properties Real Estate Investment Trust</td>\n",
       "      <td>CA0194561027</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1239481990.0</td>\n",
       "      <td>WARNING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2441048</td>\n",
       "      <td>104020601.0</td>\n",
       "      <td>AR</td>\n",
       "      <td>2019-02-13</td>\n",
       "      <td>2021-05-16 09:34:00.000</td>\n",
       "      <td>Allied Properties Real Estate Investment Trust</td>\n",
       "      <td>CA0194561027</td>\n",
       "      <td>1.0</td>\n",
       "      <td>592084468.0</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3264481</td>\n",
       "      <td>6833477.0</td>\n",
       "      <td>AR</td>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>2021-05-16 09:34:00.000</td>\n",
       "      <td>Allied Properties Real Estate Investment Trust</td>\n",
       "      <td>CA0194561027</td>\n",
       "      <td>1.0</td>\n",
       "      <td>633055763.0</td>\n",
       "      <td>WARNING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3209328</td>\n",
       "      <td>25768803.0</td>\n",
       "      <td>AR</td>\n",
       "      <td>2011-03-30</td>\n",
       "      <td>2021-05-16 09:34:00.000</td>\n",
       "      <td>Allied Properties Real Estate Investment Trust</td>\n",
       "      <td>CA0194561027</td>\n",
       "      <td>1.0</td>\n",
       "      <td>445185723.0</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412955</th>\n",
       "      <td>3506864</td>\n",
       "      <td>40209797.0</td>\n",
       "      <td>AR</td>\n",
       "      <td>2009-09-28</td>\n",
       "      <td>2021-05-16 09:34:00.000</td>\n",
       "      <td>Russell Investments Corporate Class Inc. - Rus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412956</th>\n",
       "      <td>2549094</td>\n",
       "      <td>79012810.0</td>\n",
       "      <td>AR</td>\n",
       "      <td>2018-05-28</td>\n",
       "      <td>2021-05-16 09:34:00.000</td>\n",
       "      <td>Terma A/S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412957</th>\n",
       "      <td>2975783</td>\n",
       "      <td>398723.0</td>\n",
       "      <td>AR</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2021-05-16 09:34:00.000</td>\n",
       "      <td>PT Asuransi Wahana Tata</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>WARNING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412958</th>\n",
       "      <td>2817522</td>\n",
       "      <td>54604016.0</td>\n",
       "      <td>AR</td>\n",
       "      <td>2017-10-30</td>\n",
       "      <td>2021-05-16 09:34:00.000</td>\n",
       "      <td>Hawke's Bay District Health Board</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>WARNING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412959</th>\n",
       "      <td>3989771</td>\n",
       "      <td>149725080.0</td>\n",
       "      <td>AR</td>\n",
       "      <td>2020-12-17</td>\n",
       "      <td>2021-05-16 09:34:00.000</td>\n",
       "      <td>Nedbank Private Wealth Ltd.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>WARNING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>412960 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  DOCUMENT_ID DOCUMENT_TYPE FILING_DATE  \\\n",
       "0       2533588  138980280.0            AR  2020-06-16   \n",
       "1       4036677  152637912.0            AR  2021-02-05   \n",
       "2       2441048  104020601.0            AR  2019-02-13   \n",
       "3       3264481    6833477.0            AR  2011-12-31   \n",
       "4       3209328   25768803.0            AR  2011-03-30   \n",
       "...         ...          ...           ...         ...   \n",
       "412955  3506864   40209797.0            AR  2009-09-28   \n",
       "412956  2549094   79012810.0            AR  2018-05-28   \n",
       "412957  2975783     398723.0            AR  2015-12-31   \n",
       "412958  2817522   54604016.0            AR  2017-10-30   \n",
       "412959  3989771  149725080.0            AR  2020-12-17   \n",
       "\n",
       "                    MODIFIED_AT  \\\n",
       "0       2021-05-16 09:34:00.000   \n",
       "1       2021-05-16 09:34:00.000   \n",
       "2       2021-05-16 09:34:00.000   \n",
       "3       2021-05-16 09:34:00.000   \n",
       "4       2021-05-16 09:34:00.000   \n",
       "...                         ...   \n",
       "412955  2021-05-16 09:34:00.000   \n",
       "412956  2021-05-16 09:34:00.000   \n",
       "412957  2021-05-16 09:34:00.000   \n",
       "412958  2021-05-16 09:34:00.000   \n",
       "412959  2021-05-16 09:34:00.000   \n",
       "\n",
       "                                 DETAIL_JSON.company_name DETAIL_JSON.ISIN  \\\n",
       "0                 Huisheng International Holdings Limited     KYG4643W1078   \n",
       "1          Allied Properties Real Estate Investment Trust     CA0194561027   \n",
       "2          Allied Properties Real Estate Investment Trust     CA0194561027   \n",
       "3          Allied Properties Real Estate Investment Trust     CA0194561027   \n",
       "4          Allied Properties Real Estate Investment Trust     CA0194561027   \n",
       "...                                                   ...              ...   \n",
       "412955  Russell Investments Corporate Class Inc. - Rus...              NaN   \n",
       "412956                                          Terma A/S              NaN   \n",
       "412957                            PT Asuransi Wahana Tata              NaN   \n",
       "412958                  Hawke's Bay District Health Board              NaN   \n",
       "412959                        Nedbank Private Wealth Ltd.              NaN   \n",
       "\n",
       "       DETAIL_JSON.ISIN_active DETAIL_JSON.SP_DocumentId  \\\n",
       "0                          1.0              1028107306.0   \n",
       "1                          1.0              1239481990.0   \n",
       "2                          1.0               592084468.0   \n",
       "3                          1.0               633055763.0   \n",
       "4                          1.0               445185723.0   \n",
       "...                        ...                       ...   \n",
       "412955                     nan                       nan   \n",
       "412956                     nan                       nan   \n",
       "412957                     nan                       nan   \n",
       "412958                     nan                       nan   \n",
       "412959                     nan                       nan   \n",
       "\n",
       "       DETAIL_JSON.parsing_status  \n",
       "0                         SUCCESS  \n",
       "1                         WARNING  \n",
       "2                         SUCCESS  \n",
       "3                         WARNING  \n",
       "4                         SUCCESS  \n",
       "...                           ...  \n",
       "412955                    SUCCESS  \n",
       "412956                    SUCCESS  \n",
       "412957                    WARNING  \n",
       "412958                    WARNING  \n",
       "412959                    WARNING  \n",
       "\n",
       "[412960 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Filing Reference Package ####\n",
    "# create an empty list, combined_ref, and loop through each file and combine to combined_ref\n",
    "# combined ref will be a list of dictionaries\n",
    "os.chdir(reference_path)\n",
    "combined_ref = []\n",
    "for file in os.listdir():\n",
    "    if(fnmatch.fnmatch(file, 'SP_FILING_REFERENCE*')):\n",
    "        with gzip.open(file,'rb') as ref:\n",
    "            ref = ndjson.load(ref)\n",
    "        combined_ref.extend(ref)    \n",
    " \n",
    "# normalize the reference package from its NDJSON form into a Data Frame - uses from pandas.io.json import json_normalize\n",
    "ref_normalize = pd.json_normalize(combined_ref)\n",
    "\n",
    "# change order of columns to reflect User Guide documentation\n",
    "ref_normalize = ref_normalize[['ID', 'DOCUMENT_ID','DOCUMENT_TYPE','FILING_DATE','MODIFIED_AT','DETAIL_JSON.company_name','DETAIL_JSON.ISIN', 'DETAIL_JSON.ISIN_active','DETAIL_JSON.SP_DocumentId', 'DETAIL_JSON.parsing_status']]\n",
    "\n",
    "# convert the ID and document ID field to a string\n",
    "ref_normalize['ID'] = ref_normalize['ID'].apply(str)\n",
    "ref_normalize['DOCUMENT_ID'] = ref_normalize['DOCUMENT_ID'].apply(str)\n",
    "ref_normalize['DETAIL_JSON.ISIN_active'] = ref_normalize['DETAIL_JSON.ISIN_active'].apply(str)\n",
    "ref_normalize['DETAIL_JSON.SP_DocumentId'] = ref_normalize['DETAIL_JSON.SP_DocumentId'].apply(str)\n",
    "\n",
    "ref_normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Sentiment Score Package ####\n",
    "# create an empty list, combined_sc, and loop through each file and combine to combined_sc\n",
    "# combined_sc will be a list of dictionaries\n",
    "os.chdir(sent_path)\n",
    "combined_sc = []\n",
    "for file in os.listdir():\n",
    "    if(fnmatch.fnmatch(file, 'SP_FILING_SENTIMENT*')):\n",
    "        try:\n",
    "            # with open(file,'r',encoding='utf_8') as sc:\n",
    "            with gzip.open(file,'rb') as sc:# gzip 用来打开压缩文件中的数据\n",
    "                sc = ndjson.load(sc)\n",
    "            combined_sc.extend(sc)  # 加入最后\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            flag+=1\n",
    "        continue\n",
    "\n",
    "combined_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filing Level Function\n",
    "# Function for calculating the sentiment figures at the main filing level - i.e., the aggregated sentiment figures for the actual\n",
    "# filing (EXCLUDING exhibits)\n",
    "#Parameter: filing_type - the filing type of the data in combined_sc (examples: '10-k', '10-q')\n",
    "def filing_level_func(filing_type):\n",
    "# column names of sentiment scores package based on User Guide documentation\n",
    "    column_names = ['ID','avg_sent','sum_sent','hit_count','positive_hits','negative_hits','section_count','word_count']\n",
    "# create empty list called filin g_level_list\n",
    "    filing_level_list = [[]]\n",
    "# loop through each element in combined_sc, check if the filing_type key exists in the 'SENTIMENT' key\n",
    "# if it does append doc_level_list with ID and the sentiment scores\n",
    "# if it does not, append doc_level_level with ID and NAs\n",
    "    for element in range(len(combined_sc)):\n",
    "        if combined_sc[element].get('SENTIMENT',{}).get(filing_type):\n",
    "            filing_level_list.append([str(combined_sc[element]['ID']), \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['avg_sent'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['sum_sent'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['hit_count'], \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['positive_hits'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['negative_hits'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['section_count'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['word_count']])\n",
    "        else:\n",
    "            filing_level_list.append([str(combined_sc[element]['ID']), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,np.nan])\n",
    "\n",
    "    # take contents of filing_level_list and put into a data frame called filing_level_df\n",
    "    filing_level_df = pd.DataFrame(filing_level_list[1:len(filing_level_list)], columns = column_names)    \n",
    "    \n",
    "    # convert the ID field to a string\n",
    "    filing_level_df['ID'] = filing_level_df['ID'].apply(str)\n",
    "    \n",
    "    # return the data frame\n",
    "    return filing_level_df\n",
    "\n",
    "\n",
    "## Section Level Function\n",
    "# Function for calculating the sentiment figures at the section level\n",
    "#Parameter: \n",
    "# filing_type - the filing type of the data in combined_sc (examples: 'AR', 'QR', 'SR')\n",
    "# section - the section of the filing_type of the data in combined_sc (examples: 'data', 'letter to shareholders', 'ceo report',etc)    \n",
    "def section_level_func(filing_type, section):\n",
    "# column names of sentiment scores package based on User Guide documentation\n",
    "    column_names = ['ID','avg_sent','sum_sent','hit_count','positive_hits','negative_hits','section_count','word_count']   \n",
    "# create empty list called section_level_list\n",
    "    section_level_list = [[]] \n",
    "# loop through each element in combined_sc, check if the section key exists in the 'SENTIMENT' key -> filing_type key\n",
    "# if it does append section_level_list with ID and the sentiment scores\n",
    "# if it does not, append section_level_level with ID and NAs\n",
    "    for element in range(len(combined_sc)):\n",
    "        if combined_sc[element].get('SENTIMENT',{}).get(filing_type,{}).get(section):\n",
    "            section_level_list.append([str(combined_sc[element]['ID']), \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['avg_sent'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['sum_sent'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['hit_count'], \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['positive_hits'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['negative_hits'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['section_count'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['word_count']])\n",
    "        else:\n",
    "            section_level_list.append([str(combined_sc[element]['ID']), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,np.nan])\n",
    "\n",
    "    # take contents of section_level_list and put into a data frame called section_level_df\n",
    "    section_level_df = pd.DataFrame(section_level_list[1:len(section_level_list)], columns = column_names)    \n",
    "    \n",
    "    # convert the ID field to a string\n",
    "    section_level_df['ID'] = section_level_df['ID'].apply(str)\n",
    "    \n",
    "    # return the data frame\n",
    "    return section_level_df\n",
    "\n",
    "\n",
    "## Sub-section Level Function  \n",
    "#Parameter: \n",
    "# filing_type - the filing type of the data in combined_sc (examples: '10-k', '10-q')\n",
    "# section - the section of the filing_type of the data in combined_sc (examples: 'data', 'letter to shareholders', 'ceo report',etc)    \n",
    "# sub-section - the sub-section of the section of the filing_type in combined_sc (examples: 'data','esg','risk', etc.)  \n",
    "def sub_level_func(filing_type, section, sub):\n",
    "# column names of sentiment scores package based on User Guide documentation\n",
    "    column_names = ['ID','avg_sent','sum_sent','hit_count','positive_hits','negative_hits','section_count','word_count']\n",
    "# create empty list called sub_level_list\n",
    "    sub_level_list = [[]] \n",
    "# loop through each element in combined_sc, check if the seub-section key exists in the 'SENTIMENT' key -> filing_type key -> section key\n",
    "# if it does append sub_level_list with ID and the sentiment scores\n",
    "# if it does not, append sub_level_list with ID and NAs\n",
    "    for element in range(len(combined_sc)):\n",
    "        if combined_sc[element].get('SENTIMENT',{}).get(filing_type,{}).get(section,{}).get(sub):\n",
    "            sub_level_list.append([str(combined_sc[element]['ID']), \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['avg_sent'], \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['sum_sent'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['hit_count'], \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['positive_hits'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['negative_hits'], \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['section_count'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['word_count']])\n",
    "      \n",
    "        else: sub_level_list.append([str(combined_sc[element]['ID']), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,np.nan])\n",
    "        \n",
    "    # take contents of sub_level_list and put into a data frame called sub_level_df\n",
    "    sub_level_df = pd.DataFrame(sub_level_list[1:len(sub_level_list)], columns = column_names)    \n",
    "    \n",
    "    # convert the ID field to a string\n",
    "    sub_level_df['ID'] = sub_level_df['ID'].apply(str)\n",
    "    \n",
    "    # return the data frame\n",
    "    return sub_level_df\n",
    "\n",
    "## Examples of calling functions\n",
    "# get the AR sentiment data at the filing level\n",
    "ars = filing_level_func('ar')\n",
    "\n",
    "## Get just the documents for active companies with their active ISINs or non-active companies with all their ISINs\n",
    "# Count the number of ISINs that are active and non-active for each document_id\n",
    "doc_id_table = pd.crosstab(index = ref_normalize['DOCUMENT_ID'], columns = ref_normalize['DETAIL_JSON.ISIN_active'])\n",
    "\n",
    "# docs with active ISINs have more than 0 in 1.0 column\n",
    "actives = doc_id_table[doc_id_table['1.0'] > 0]\n",
    "# docs from inactive companies have more than 0 in 0.0 column and exactly 0 in 1.0 column\n",
    "non_actives = doc_id_table[(doc_id_table['0.0'] > 0) & (doc_id_table['1.0'] == 0)]\n",
    "\n",
    "# add flags to ref_normalize\n",
    "ref_normalize['active'] = ref_normalize['DOCUMENT_ID'].isin(actives.index).astype(int)\n",
    "ref_normalize['not_active'] = ref_normalize['DOCUMENT_ID'].isin(non_actives.index).astype(int)\n",
    "\n",
    "# filter ref_normalize active companies and just their active ISINs or inactive companies with all their ISINs\n",
    "onlyActive_allNonActive = ref_normalize[(ref_normalize['active'] == 1) & (ref_normalize['DETAIL_JSON.ISIN_active'] == '1.0') | ref_normalize['not_active'] == 1]\n",
    "\n",
    "# Join reference information with sentiment metrics\n",
    "combined = pd.merge(left = onlyActive_allNonActive, right = ars, on = 'ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine how many data that the exception drops\n",
    "print(\"Json_error_number: \"+str(flag))\n",
    "\n",
    "# drop the duplicate \n",
    "len_init=len(combined)\n",
    "print(\"origin_line_number: \"+str(len_init))\n",
    "combined.duplicated(['ID'])\n",
    "output=combined.drop_duplicates(['ID'])\n",
    "len_out=len(output)\n",
    "drop_len=len_init-len_out\n",
    "print(\"drop_duplicate_lines_number: \"+str(drop_len))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "203da323273579ec230429ec83021f717a0b02c3371af76a5d76ba027b98163f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
