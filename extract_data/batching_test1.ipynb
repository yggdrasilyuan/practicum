{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\zip_test\\\\REFERENCE'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ndjson\n",
    "import os\n",
    "import gzip\n",
    "import fnmatch\n",
    "\n",
    "\n",
    "# folder with the Reference, Data, Word Count, and Sentiment zipped files\n",
    "# os.chdir('D:\\\\CA\\\\en-US')\n",
    "path=\"F:\\\\zip_test\"\n",
    "reference_path=path+\"\\\\REFERENCE\"\n",
    "sent_path=path+\"\\\\SENTIMENT\"\n",
    "flag=0\n",
    "\n",
    "reference_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'DETAIL_JSON.SP_DocumentId'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32me:\\anaconda\\envs\\practicum\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda\\envs\\practicum\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32me:\\anaconda\\envs\\practicum\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'DETAIL_JSON.SP_DocumentId'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22024\\75667625.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mref_normalize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DOCUMENT_ID'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mref_normalize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DOCUMENT_ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# ref_normalize['DETAIL_JSON.ISIN_active'] = ref_normalize['DETAIL_JSON.ISIN_active'].apply(str)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mref_normalize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DETAIL_JSON.SP_DocumentId'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mref_normalize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DETAIL_JSON.SP_DocumentId'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mref_normalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda\\envs\\practicum\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda\\envs\\practicum\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'DETAIL_JSON.SP_DocumentId'"
     ]
    }
   ],
   "source": [
    "#### Filing Reference Package ####\n",
    "# create an empty list, combined_ref, and loop through each file and combine to combined_ref\n",
    "# combined ref will be a list of dictionaries\n",
    "os.chdir(reference_path)\n",
    "combined_ref = []\n",
    "for file in os.listdir(reference_path):\n",
    "    # print(file)\n",
    "    if(fnmatch.fnmatch(file, 'SP_FILING_REFERENCE*')):\n",
    "        # with open(file,'r',encoding='utf_8') as ref:\n",
    "        with gzip.open(file,'rb') as ref:\n",
    "            ref = ndjson.load(ref)\n",
    "        combined_ref.extend(ref)    \n",
    " \n",
    "# normalize the reference package from its NDJSON form into a Data Frame - uses from pandas.io.json import json_normalize\n",
    "ref_normalize = pd.json_normalize(combined_ref)\n",
    "\n",
    "# change order of columns to reflect User Guide documentation\n",
    "ref_normalize = ref_normalize[['ID', 'DOCUMENT_ID','DOCUMENT_TYPE','FILING_DATE','MODIFIED_AT','DETAIL_JSON.company_name','DETAIL_JSON.ISIN_active' ,'DETAIL_JSON.SP_DocumentId','DETAIL_JSON.parsing_status']]\n",
    "\n",
    "# convert the ID and document ID field to a string\n",
    "ref_normalize['ID'] = ref_normalize['ID'].apply(str)\n",
    "ref_normalize['DOCUMENT_ID'] = ref_normalize['DOCUMENT_ID'].apply(str)\n",
    "ref_normalize['DETAIL_JSON.ISIN_active'] = ref_normalize['DETAIL_JSON.ISIN_active'].apply(str)\n",
    "ref_normalize['DETAIL_JSON.SP_DocumentId'] = ref_normalize['DETAIL_JSON.SP_DocumentId'].apply(str)\n",
    "\n",
    "ref_normalize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Sentiment Score Package ####\n",
    "# create an empty list, combined_sc, and loop through each file and combine to combined_sc\n",
    "# combined_sc will be a list of dictionaries\n",
    "os.chdir(sent_path)\n",
    "combined_sc = []\n",
    "for file in os.listdir():\n",
    "    if(fnmatch.fnmatch(file, 'SP_FILING_SENTIMENT*')):\n",
    "        try:\n",
    "            # with open(file,'r',encoding='utf_8') as sc:\n",
    "            with gzip.open(file,'rb') as sc:# gzip 用来打开压缩文件中的数据\n",
    "                sc = ndjson.load(sc)\n",
    "            combined_sc.extend(sc)  # 加入最后\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            flag+=1\n",
    "        continue\n",
    "\n",
    "combined_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Filing Level Function\n",
    "# Function for calculating the sentiment figures at the main filing level - i.e., the aggregated sentiment figures for the actual\n",
    "# filing (EXCLUDING exhibits)\n",
    "#Parameter: filing_type - the filing type of the data in combined_sc (examples: '10-k', '10-q')\n",
    "def filing_level_func(filing_type):\n",
    "# column names of sentiment scores package based on User Guide documentation\n",
    "    column_names = ['ID','avg_sent','sum_sent','hit_count','positive_hits','negative_hits','section_count','word_count']\n",
    "# create empty list called filin g_level_list\n",
    "    filing_level_list = [[]]\n",
    "# loop through each element in combined_sc, check if the filing_type key exists in the 'SENTIMENT' key\n",
    "# if it does append doc_level_list with ID and the sentiment scores\n",
    "# if it does not, append doc_level_level with ID and NAs\n",
    "    for element in range(len(combined_sc)):\n",
    "        if combined_sc[element].get('SENTIMENT',{}).get(filing_type):\n",
    "            filing_level_list.append([str(combined_sc[element]['ID']), \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['avg_sent'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['sum_sent'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['hit_count'], \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['positive_hits'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['negative_hits'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['section_count'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['word_count']])\n",
    "        else:\n",
    "            filing_level_list.append([str(combined_sc[element]['ID']), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,np.nan])\n",
    "\n",
    "    # take contents of filing_level_list and put into a data frame called filing_level_df\n",
    "    filing_level_df = pd.DataFrame(filing_level_list[1:len(filing_level_list)], columns = column_names)    \n",
    "     \n",
    "    # convert the ID field to a string\n",
    "    filing_level_df['ID'] = filing_level_df['ID'].apply(str)\n",
    "    \n",
    "    # return the data frame\n",
    "    return filing_level_df\n",
    "\n",
    "\n",
    "## Section Level Function\n",
    "# Function for calculating the sentiment figures at the section level\n",
    "#Parameter: \n",
    "# filing_type - the filing type of the data in combined_sc (examples: 'AR', 'QR', 'SR')\n",
    "# section - the section of the filing_type of the data in combined_sc (examples: 'data', 'letter to shareholders', 'ceo report',etc)    \n",
    "def section_level_func(filing_type, section):\n",
    "# column names of sentiment scores package based on User Guide documentation\n",
    "    column_names = ['ID','avg_sent','sum_sent','hit_count','positive_hits','negative_hits','section_count','word_count']   \n",
    "# create empty list called section_level_list\n",
    "    section_level_list = [[]] \n",
    "# loop through each element in combined_sc, check if the section key exists in the 'SENTIMENT' key -> filing_type key\n",
    "# if it does append section_level_list with ID and the sentiment scores\n",
    "# if it does not, append section_level_level with ID and NAs\n",
    "    for element in range(len(combined_sc)):\n",
    "        if combined_sc[element].get('SENTIMENT',{}).get(filing_type,{}).get(section):\n",
    "            section_level_list.append([str(combined_sc[element]['ID']), \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['avg_sent'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['sum_sent'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['hit_count'], \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['positive_hits'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['negative_hits'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['section_count'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['word_count']])\n",
    "        else:\n",
    "            section_level_list.append([str(combined_sc[element]['ID']), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,np.nan])\n",
    "\n",
    "    # take contents of section_level_list and put into a data frame called section_level_df\n",
    "    section_level_df = pd.DataFrame(section_level_list[1:len(section_level_list)], columns = column_names)    \n",
    "    \n",
    "    # convert the ID field to a string\n",
    "    section_level_df['ID'] = section_level_df['ID'].apply(str)\n",
    "    \n",
    "    # return the data frame\n",
    "    return section_level_df\n",
    "\n",
    "\n",
    "## Sub-section Level Function  \n",
    "#Parameter: \n",
    "# filing_type - the filing type of the data in combined_sc (examples: '10-k', '10-q')\n",
    "# section - the section of the filing_type of the data in combined_sc (examples: 'data', 'letter to shareholders', 'ceo report',etc)    \n",
    "# sub-section - the sub-section of the section of the filing_type in combined_sc (examples: 'data','esg','risk', etc.)  \n",
    "def sub_level_func(filing_type, section, sub):\n",
    "# column names of sentiment scores package based on User Guide documentation\n",
    "    column_names = ['ID','avg_sent','sum_sent','hit_count','positive_hits','negative_hits','section_count','word_count']\n",
    "# create empty list called sub_level_list\n",
    "    sub_level_list = [[]] \n",
    "# loop through each element in combined_sc, check if the seub-section key exists in the 'SENTIMENT' key -> filing_type key -> section key\n",
    "# if it does append sub_level_list with ID and the sentiment scores\n",
    "# if it does not, append sub_level_list with ID and NAs\n",
    "    for element in range(len(combined_sc)):\n",
    "        if combined_sc[element].get('SENTIMENT',{}).get(filing_type,{}).get(section,{}).get(sub):\n",
    "            sub_level_list.append([str(combined_sc[element]['ID']), \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['avg_sent'], \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['sum_sent'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['hit_count'], \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['positive_hits'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['negative_hits'], \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['section_count'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['word_count']])\n",
    "      \n",
    "        else: sub_level_list.append([str(combined_sc[element]['ID']), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,np.nan])\n",
    "        \n",
    "    # take contents of sub_level_list and put into a data frame called sub_level_df\n",
    "    sub_level_df = pd.DataFrame(sub_level_list[1:len(sub_level_list)], columns = column_names)    \n",
    "    \n",
    "    # convert the ID field to a string\n",
    "    sub_level_df['ID'] = sub_level_df['ID'].apply(str)  \n",
    "    \n",
    "    # return the data frame\n",
    "    return sub_level_df\n",
    "\n",
    "## Examples of calling functions\n",
    "# get the AR sentiment data at the filing level\n",
    "ars = filing_level_func('ar')\n",
    "\n",
    "# Join reference information with sentiment metrics\n",
    "combined = pd.merge(left = ref_normalize, right = ars, on = 'ID')\n",
    "\n",
    "# Examine how many data that the exception drops\n",
    "print(\"Json_error_number: \"+str(flag))\n",
    "\n",
    "# drop the duplicate \n",
    "len_init=len(combined)\n",
    "print(\"origin_line_number: \"+str(len_init))\n",
    "combined.duplicated(['ID'])\n",
    "output=combined.drop_duplicates(['ID'])\n",
    "len_out=len(output)\n",
    "drop_len=len_init-len_out\n",
    "print(\"drop_duplicate_lines_number: \"+str(drop_len))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3e1d988fd02f3af9046135819fd37ed5d41f46c7de55723b70daeaf254c35f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
